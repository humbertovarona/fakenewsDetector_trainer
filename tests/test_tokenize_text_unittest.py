
# Generated by CodiumAI

import unittest

class TestTokenizeText(unittest.TestCase):

    #  The function should tokenize the text data correctly.
    def test_tokenize_text_tokenization(self):
        # Arrange
        X_train = ["This is a test sentence.", "Another test sentence."]
        X_test = ["Yet another test sentence."]
        max_words = 10000
        max_sequence_length = 200
    
        # Act
        X_train_seq, X_test_seq = tokenize_text(X_train, X_test, max_words, max_sequence_length)
    
        # Assert
        self.assertEqual(len(X_train_seq), len(X_train))
        self.assertEqual(len(X_test_seq), len(X_test))
        self.assertEqual(X_train_seq[0], [1, 2, 3, 4, 5])
        self.assertEqual(X_train_seq[1], [6, 7, 3, 4, 5])
        self.assertEqual(X_test_seq[0], [8, 6, 7, 3, 4, 5])

    #  The function should pad the sequences correctly.
    def test_tokenize_text_padding(self):
        # Arrange
        X_train = ["This is a test sentence.", "Another test sentence."]
        X_test = ["Yet another test sentence."]
        max_words = 10000
        max_sequence_length = 10
    
        # Act
        X_train_seq, X_test_seq = tokenize_text(X_train, X_test, max_words, max_sequence_length)
    
        # Assert
        self.assertEqual(len(X_train_seq[0]), max_sequence_length)
        self.assertEqual(len(X_train_seq[1]), max_sequence_length)
        self.assertEqual(len(X_test_seq[0]), max_sequence_length)

    #  The function should return two sequences of equal length.
    def test_tokenize_text_equal_length(self):
        # Arrange
        X_train = ["This is a test sentence.", "Another test sentence."]
        X_test = ["Yet another test sentence."]
        max_words = 10000
        max_sequence_length = 200
    
        # Act
        X_train_seq, X_test_seq = tokenize_text(X_train, X_test, max_words, max_sequence_length)
    
        # Assert
        self.assertEqual(len(X_train_seq), len(X_test_seq))

    #  The function should handle input strings that exceed the maximum sequence length.
    def test_tokenize_text_long_sequence(self):
        # Arrange
        X_train = ["This is a very long sentence that exceeds the maximum sequence length."]
        X_test = ["Another very long sentence that exceeds the maximum sequence length."]
        max_words = 10000
        max_sequence_length = 10
    
        # Act
        X_train_seq, X_test_seq = tokenize_text(X_train, X_test, max_words, max_sequence_length)
    
        # Assert
        self.assertEqual(len(X_train_seq[0]), max_sequence_length)
        self.assertEqual(len(X_test_seq[0]), max_sequence_length)

    #  The function should handle input strings that exceed the maximum number of words.
    def test_tokenize_text_long_words(self):
        # Arrange
        X_train = ["This is a sentence with a lot of words that exceeds the maximum number of words."]
        X_test = ["Another sentence with a lot of words that exceeds the maximum number of words."]
        max_words = 10
        max_sequence_length = 200
    
        # Act
        X_train_seq, X_test_seq = tokenize_text(X_train, X_test, max_words, max_sequence_length)
    
        # Assert
        self.assertEqual(len(X_train_seq[0]), max_sequence_length)
        self.assertEqual(len(X_test_seq[0]), max_sequence_length)

    #  The function should handle input strings that contain non-ASCII characters.
    def test_tokenize_text_non_ascii(self):
        # Arrange
        X_train = ["This is a sentence with non-ASCII characters: éàè"]
        X_test = ["Another sentence with non-ASCII characters: ñü"]
        max_words = 10000
        max_sequence_length = 200
    
        # Act
        X_train_seq, X_test_seq = tokenize_text(X_train, X_test, max_words, max_sequence_length)
    
        # Assert
        self.assertEqual(len(X_train_seq[0]), max_sequence_length)
        self.assertEqual(len(X_test_seq[0]), max_sequence_length)
